{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = tf.keras.utils.get_file('train.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt')\n",
    "path_to_test = tf.keras.utils.get_file('test.txt', 'https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = open(path_to_train, 'rb').read().decode(encoding='utf-8')\n",
    "test_text = open(path_to_test, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = KeyedVectors.load('./data/emb_en2ko_noun.w2v')\n",
    "tmp_2 = KeyedVectors.load('./data/emb_en2ko_verb.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('seoulh', 0.899796187877655),\n",
       " ('seoulite', 0.8980000019073486),\n",
       " ('seoun', 0.8858004808425903),\n",
       " ('seoula', 0.8574132323265076),\n",
       " ('seoultech', 0.8443858623504639),\n",
       " ('seoulbus', 0.8298246264457703),\n",
       " ('seoulmetro', 0.8171312808990479),\n",
       " ('cineseoul', 0.8098125457763672),\n",
       " ('seokbul', 0.8047872185707092),\n",
       " ('seoulfortress', 0.7911849021911621),\n",
       " ('seoil', 0.789262056350708),\n",
       " ('seouruiryowon', 0.789069414138794),\n",
       " ('seokjeon', 0.7855755686759949),\n",
       " ('seonamsa', 0.7844101190567017),\n",
       " ('seogu', 0.7815464735031128),\n",
       " ('seonam', 0.7800287008285522),\n",
       " ('sportsseoul', 0.778380274772644),\n",
       " ('seongpo', 0.777580976486206),\n",
       " ('seon', 0.7766729593276978),\n",
       " ('seosan', 0.7757962942123413),\n",
       " ('seokyungjoo', 0.7729594707489014),\n",
       " ('seoksu', 0.7723531723022461),\n",
       " ('seongnam', 0.7723338007926941),\n",
       " ('seoktap', 0.7715747952461243),\n",
       " ('seokgye', 0.7690640687942505),\n",
       " ('seok', 0.7689398527145386),\n",
       " ('seohocheon', 0.7686943411827087),\n",
       " ('seonjeongneung', 0.7658331990242004),\n",
       " ('seoulutd', 0.7622900009155273),\n",
       " ('seomyeon', 0.7621059417724609),\n",
       " ('jiseok', 0.7618528604507446),\n",
       " ('seongsan', 0.7599871754646301),\n",
       " ('seongan', 0.759575366973877),\n",
       " ('seojeong', 0.7591200470924377),\n",
       " ('seongju', 0.7590125203132629),\n",
       " ('gyeongsangnamdo', 0.7578539848327637),\n",
       " ('kyungnam', 0.7577040195465088),\n",
       " ('cheonggyecheon', 0.7563184499740601),\n",
       " ('seodaemun', 0.7557417154312134),\n",
       " ('suseonggucheong', 0.7551701664924622),\n",
       " ('hongcheon', 0.7550124526023865),\n",
       " ('yeongnamroo', 0.7546387910842896),\n",
       " ('seocheon', 0.7539432048797607),\n",
       " ('gu_seoul', 0.75347900390625),\n",
       " ('서울시지식공유', 0.7519313097000122),\n",
       " ('seongdong', 0.7516613006591797),\n",
       " ('yongsan', 0.7513471245765686),\n",
       " ('seonpyeong', 0.7506278157234192),\n",
       " ('sukjeongmun', 0.750206708908081),\n",
       " ('jukjeon', 0.7501943111419678)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_2.wv.most_similar('seoul', topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '진짜', '짜증나네요', '목소리']\n",
      "['흠', '포스터보고', '초딩영화줄', '오버연기조차', '가볍지', '않구나']\n",
      "['너무재밓었다그래서보는것을추천한다']\n",
      "['교도소', '이야기구먼', '솔직히', '재미는', '없다', '평점', '조정']\n",
      "['사이몬페그의', '익살스런', '연기가', '돋보였던', '영화', '!', '스파이더맨에서', '늙어보이기만', '했던', '커스틴', '던스트가', '너무나도', '이뻐보였다']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^가-힣A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"\\'{2,}\", \"\\'\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    return string.lower()\n",
    "train_text_x = [row.split('\\t')[1] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0]\n",
    "train_text_x = [clean_str(sentence) for sentence in train_text_x]\n",
    "sentences = [sentence.split(' ') for sentence in train_text_x]\n",
    "for i in range(5):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['아 더빙 진짜 짜증나네요 목소리',\n",
       " '흠 포스터보고 초딩영화줄 오버연기조차 가볍지 않구나',\n",
       " '너무재밓었다그래서보는것을추천한다',\n",
       " '교도소 이야기구먼 솔직히 재미는 없다 평점 조정',\n",
       " '사이몬페그의 익살스런 연기가 돋보였던 영화 ! 스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
       " '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화 별반개도 아까움 ',\n",
       " '원작의 긴장감을 제대로 살려내지못했다 ',\n",
       " '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지 정말 발로해도 그것보단 낫겟다 납치 감금만반복반복 이드라마는 가족도없다 연기못하는사람만모엿네',\n",
       " '액션이 없는데도 재미 있는 몇안되는 영화',\n",
       " '왜케 평점이 낮은건데 \\\\? 꽤 볼만한데 헐리우드식 화려함에만 너무 길들여져 있나 \\\\? ',\n",
       " '걍인피니트가짱이다 진짜짱이다 ',\n",
       " '볼때마다 눈물나서 죽겠다90년대의 향수자극 ! ! 허진호는 감성절제멜로의 달인이다 ',\n",
       " '울면서 손들고 횡단보도 건널때 뛰쳐나올뻔 이범수 연기 드럽게못해',\n",
       " '담백하고 깔끔해서 좋다 신문기사로만 보다 보면 자꾸 잊어버린다 그들도 사람이었다는 것을 ',\n",
       " '취향은 존중한다지만 진짜 내생에 극장에서 본 영화중 가장 노잼 노감동임 스토리도 어거지고 감동도 어거지',\n",
       " ' 냥 매번 긴장되고 재밋음 ',\n",
       " '참 사람들 웃긴게 바스코가 이기면 락스코라고 까고바비가 이기면 아이돌이라고 깐다 그냥 까고싶어서 안달난것처럼 보인다',\n",
       " '굿바이 레닌 표절인것은 이해하는데 왜 뒤로 갈수록 재미없어지냐',\n",
       " '이건 정말 깨알 캐스팅과 질퍽하지않은 산뜻한 내용구성이 잘 버무러진 깨알일드 ! ! ',\n",
       " '약탈자를 위한 변명 , 이라 저놈들은 착한놈들 절대 아닌걸요 ',\n",
       " '나름 심오한 뜻도 있는 듯 그냥 학생이 선생과 놀아나는 영화는 절대 아님',\n",
       " '보면서 웃지 않는 건 불가능하다',\n",
       " '재미없다 지루하고 같은 음식 영화인데도 바베트의 만찬하고 넘 차이남 바베트의 만찬은 이야기도 있고 음식 보는재미도 있는데 이건 볼게없다 음식도 별로 안나오고 , 핀란드 풍경이라도 구경할랫는데 그것도 별로 안나옴 ',\n",
       " '절대 평범한 영화가 아닌 수작이라는걸 말씀드립니다 ',\n",
       " '주제는 좋은데 중반부터 지루하다',\n",
       " '다 짤랐을꺼야 그래서 납득할 수 없었던거야 그럴꺼야 꼭 그랬던걸꺼야 ',\n",
       " 'kl2g 고추를 털어버려야 할텐데',\n",
       " '카밀라벨 발연기',\n",
       " '재밋는뎅',\n",
       " '센스있는 연출력 탁월한 캐스팅 90년대의 향수 그래서 9점 ',\n",
       " '엄포스의 위력을 다시 한번 깨닫게 해준 적 남 꽃검사님도 연기 정말 좋았어요 ! 완전 명품드라마 ! ',\n",
       " '졸쓰레기 진부하고말도안됌 아 시간아까워',\n",
       " '재밌는데 별점이 왜이리 낮은고',\n",
       " '1 라도 기대했던 내가 죄인입니다 죄인입니다 ',\n",
       " '아직도 이 드라마는 내인생의 최고 ! ',\n",
       " '패션에 대한 열정 ! 안나 윈투어 ! ',\n",
       " '키이라 나이틀리가 연기하고자 했던건 대체 정신장애일까 틱장애일까',\n",
       " '허허 원작가 정신나간 유령이라 재미있겠네요 ! ',\n",
       " '포스터는 있어보이는데 관객은 114명이네',\n",
       " '이 영화가 왜 이렇게 저평가 받는지 모르겠다',\n",
       " '단순하면서 은은한 매력의 영화',\n",
       " '다 알바생인가 내용도 없고 무서운거도 없고 웃긴거도 하나도 없음 완전 별싱거운 영화 내 시간 넘 아까움 완전 낚임',\n",
       " '오게두어라 ! 서리한이 굶주렸다 ! ',\n",
       " '정말 맘에 들어요 그래서 또 보고싶은데 또 보는 방법이 없네 \\\\? ',\n",
       " '윤제문이라는 멋진 배우를 발견하게 됐어요 소소한 일탈이 잔잔한 미소를 머금게 합니다 음악은 조금 아쉽네요 8점 주고 싶은데 평점 올리고 싶어 10점 줄게요 ',\n",
       " '평점에속지마시길시간낭비 돈낭비임',\n",
       " '리얼리티가 뛰어나긴 한데 큰 공감은 안간다 이민기캐릭터는 정신의학상 분노조절장애 초기 증상일거다 툭하면 사람패고 욕하고 물건 파손하고 조금 오바였음 극 초반엔 신선했는데 가면 갈수록 이민기 정신상태 공감불가 ',\n",
       " '마이너스는 왜없냐 뮤비 보고 영화수준 딱 알만하더군 북한에서 이런거 만들라고 돈 대주던 \\\\? ',\n",
       " '난 우리영화를 사랑합니다 ',\n",
       " '데너리스 타르 가르엔 나도 용의주인이 되고 싶다 누이랑 , 근친상간이나 하고 다닐지라도 , 소설 속에선 제일 멋진 놈이 자이메 라니스터였는데 , 드라마속에선 , 드래곤 \\\\( 용 \\\\) 이 제일 멋지네 \\\\( 웃음 \\\\) 감독님 토르 2 다크 월드는 말아 잡수셨을지라도 , 기본 선방은 했음',\n",
       " '영화가 사람의 영혼을 어루만져 줄 수도 있군요 거친 세상사를 잠시 잊고 동화같은 영화에 행복했네요',\n",
       " '야 세르게이 ! 작은고추의 매운맛을 보여주마 ! 포퐁저그 콩진호가 간다',\n",
       " '이렇게 가슴시리게 본 드라마가 또 있을까 \\\\? 감동 그 자체 ! ',\n",
       " '난또 저 꼬마애가 무슨 원한이 깊길래 , 했더니 oo 그냥 혼자 나대다 oo걸 어쩌라고 ',\n",
       " '재미있어요',\n",
       " '전 좋아요',\n",
       " '최고',\n",
       " '너무 충격적이엇다 기분을 완전히 푹 꺼지게 하는 느낌 활력이라고는 하나도 없는 너무나도 무거운 지독하고 차갑고 무자비하다 그저 일본인들의 상상력은 정말 대단한거 같다는 생각이 든다 ',\n",
       " '심심한영화 ',\n",
       " '백봉기 언제나오나요 \\\\? ',\n",
       " '보는내내 그대로 들어맞는 예측 카리스마 없는 악역',\n",
       " '불알이 나와서 당황 아무튼 영화가 중간에 끝나는 느낌',\n",
       " '평범함속에 녹아든 평범한 일상 조금 밋밋한게 흠 ',\n",
       " '보던거라 계속보고있는데 전개도 느리고 주인공인 은희는 한두컷 나오면서 소극적인모습에 짜증이 맨날 언제끝나나 기대만하고있어요 전개좀 빨리빨리 ',\n",
       " '사랑하고싶게하는 , 가슴속온감정을헤집어놓는영화예요정말최고 ',\n",
       " '많은 사람들이 이 다큐를 보고 우리나라 슬픈 현대사의 한 단면에 대해 깊이 생각하고 사죄하고 바로 잡기 위해 노력했으면 합니다 말로만 듣던 보도연맹 , 그 민간인 학살이 이정도 일 줄이야 이건 명백한 살인입니다 살인자들은 다 어디있나요 \\\\? ',\n",
       " '예전 작품 캐릭터 , 에피소드 재탕 삼탕 사골우려먹듯 우리고 내용은 산으로 가고 시청률은 아예안나오고 이제 70회중반인데 120부작이라니 ',\n",
       " '김남길의 백점짜리 연기력과 초반 몰입도에도 불구하고 지루하고 손예진 ',\n",
       " '재밌네 비슷한 영화를 안보신 분들한테는 재미있을 듯',\n",
       " '노래실력으로뽑는게 맞냐 \\\\? 박시환이 mama나가면 진짜 망신이다',\n",
       " '아 일본영화 다이런건가 \\\\? \\\\? 유치하다',\n",
       " '이틀만에 다 봤어요 재밌어요 근데 차 안에 물건 넣어 조작하려고 하면 차 안이 열려있다던지 집 안이 활짝 열려서 아무나 들어간다던가 문자를 조작하려고하면 비번이 안 걸려있고 그런 건 억지스러웠는데 그래도 내용 자체는 좋았어요',\n",
       " '졸작',\n",
       " '재밋네요 달팽이가 빨라서 더 재밌었어요',\n",
       " '어설픈 전개 어이없는 결말',\n",
       " '부패한 로마노프 왕조를 기리는 뭣같은 영화 온몸으로 항거했던 러시아 민중들이 그저 폭도냐',\n",
       " '내용전개는 무난한 편이였구 잘 보았습니다 ',\n",
       " '매우 실망 ',\n",
       " '한국영화 흥행코드 갈등 갈등 계 에속 갈등 화해 감동 평점 10점 남발 흥행 뻔하지 뭐 ',\n",
       " '아햏햏 아햏햏 아햏햏 ',\n",
       " '뭐냐 시작하고 3분만에 나왔다 리플릿 사진 보며 불안하더니만 ',\n",
       " '단연 최고라고 할수있지',\n",
       " '감독이 럼먹고 영화를 만들었나보다 관객에게 뭘 말하는지도 모르겠고 , 엉망진창 개진창이다 ',\n",
       " '이건 뭐냐 \\\\? 우뢰매냐 \\\\? ',\n",
       " '정말쓰레기영화입니다',\n",
       " '진정 위대한 영화 최고임',\n",
       " '별루 였다 ',\n",
       " '내일이 기대되는 `',\n",
       " '근데 조미가 막문위 좋아한건가요 \\\\? \\\\? ',\n",
       " ' 진짜 골깜 눈 부라릴때 쓰러짐 ',\n",
       " '성룡영화중 최악인듯 ',\n",
       " '골때리네 걸스데이 이혜리 잘 되라 ! ',\n",
       " '서기가이뻐서',\n",
       " '완전 재밌어요 백인공주귀여움 ',\n",
       " '인상적인 영화였다',\n",
       " '어내스트와 셀레스틴 완전 강추에요 정말 재밌습니다 ',\n",
       " '재미있는영화입니다 ',\n",
       " '클라라볼라고화신본거아닌데',\n",
       " '진짜 보면서 너무 슬펐던 영화다',\n",
       " '설정이 재밌고 새로운 에피소드 내에서 메인 스토리도 차차 나오는게 재밌음']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_x[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "train_x = tokenizer.texts_to_sequences(sentences)\n",
    "train_x = pad_sequences(train_x, padding='post', maxlen=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array([[int(row.split('\\t')[2])] for row in train_text.split('\\n')[1:] if row.count('\\t') > 0])\n",
    "test_y = np.array([[int(row.split('\\t')[2])] for row in test_text.split('\\n')[1:] if row.count('\\t') > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 25)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.index_word) + 1\n",
    "EMBEDDING_DIM = 300\n",
    "INPUT_LENGTH = 25\n",
    "\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# tokenizer에 있는 단어 사전을 순회하면서 word2vec의 300차원 vector를 가져옵니다\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    embedding_vector = tmp[word] if word in tmp else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 만들어야함 \n",
    "input_x = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tmp_list = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            tmp_list.append(tmp.get_vector(word))\n",
    "        except:\n",
    "            tmp_list.append((np.zeros((300, ))))\n",
    "            \n",
    "    while len(tmp_list) < 25:\n",
    "        tmp_list.append((np.zeros((300, ))))\n",
    "    \n",
    "    input_x.append(tmp_list[:25])\n",
    "    \n",
    "input_x = np.array(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(25, 300)))\n",
    "model.add(tf.keras.layers.LSTM(256, recurrent_dropout=0.1))\n",
    "model.add(tf.keras.layers.Dropout(0.25))\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 586,946\n",
      "Trainable params: 586,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 25)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120000 samples, validate on 30000 samples\n",
      "Epoch 1/50\n",
      "120000/120000 [==============================] - 10s 87us/sample - loss: 0.1486 - accuracy: 0.9186 - val_loss: 0.9299 - val_accuracy: 0.7622\n",
      "Epoch 2/50\n",
      "120000/120000 [==============================] - 10s 86us/sample - loss: 0.1474 - accuracy: 0.9194 - val_loss: 0.9663 - val_accuracy: 0.7656\n",
      "Epoch 3/50\n",
      "120000/120000 [==============================] - 10s 85us/sample - loss: 0.1448 - accuracy: 0.9206 - val_loss: 0.9553 - val_accuracy: 0.7669\n",
      "Epoch 4/50\n",
      "120000/120000 [==============================] - 10s 85us/sample - loss: 0.1448 - accuracy: 0.9204 - val_loss: 1.0370 - val_accuracy: 0.7659\n",
      "Epoch 5/50\n",
      "120000/120000 [==============================] - 10s 85us/sample - loss: 0.1427 - accuracy: 0.9212 - val_loss: 0.9198 - val_accuracy: 0.7668\n",
      "Epoch 6/50\n",
      "120000/120000 [==============================] - 10s 86us/sample - loss: 0.1435 - accuracy: 0.9208 - val_loss: 0.9621 - val_accuracy: 0.7616\n",
      "Epoch 7/50\n",
      "120000/120000 [==============================] - 10s 84us/sample - loss: 0.1412 - accuracy: 0.9213 - val_loss: 1.0493 - val_accuracy: 0.7614\n",
      "Epoch 8/50\n",
      "120000/120000 [==============================] - 11s 88us/sample - loss: 0.1412 - accuracy: 0.9219 - val_loss: 1.0307 - val_accuracy: 0.7601\n",
      "Epoch 9/50\n",
      " 24064/120000 [=====>........................] - ETA: 7s - loss: 0.1351 - accuracy: 0.9255"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7f82346a49d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(input_x, train_y, epochs=50, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Way\n",
      "Back\n",
      "wants\n",
      "to\n",
      "be\n",
      "Manchester\n",
      "by\n",
      "the\n",
      "Sea\n",
      "on\n",
      "the\n",
      "hardwood,\n",
      "but\n",
      "it's\n",
      "more\n",
      "like\n",
      "Hoosiers\n",
      "with\n",
      "a\n",
      "hangover.\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"The Way Back wants to be Manchester by the Sea on the hardwood, but it's more like Hoosiers with a hangover.\"\n",
    "input_sentence = input_sentence.split(' ')\n",
    "sample_x = []\n",
    "\n",
    "for sentence in [input_sentence]:\n",
    "    tmp_list = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            print(word)\n",
    "            tmp_list.append(tmp.get_vector(word))\n",
    "        except:\n",
    "            tmp_list.append((np.zeros((300, ))))\n",
    "            \n",
    "    while len(tmp_list) < 25:\n",
    "        tmp_list.append((np.zeros((300, ))))\n",
    "    \n",
    "    sample_x.append(tmp_list[:25])\n",
    "    \n",
    "sample_x = np.array(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8839754 , 0.11602467]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/SST-2/dev.tsv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_x = [i.split(' ') for i in list(data['sentence'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = []\n",
    "\n",
    "for sentence in eval_x:\n",
    "    tmp_list = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            tmp_list.append(tmp.get_vector(word))\n",
    "        except:\n",
    "            tmp_list.append((np.zeros((300, ))))\n",
    "            \n",
    "    while len(tmp_list) < 25:\n",
    "        tmp_list.append((np.zeros((300, ))))\n",
    "    \n",
    "    x_eval.append(tmp_list[:25])\n",
    "    \n",
    "x_eval = np.array(x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 25, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_y = list(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872/872 [==============================] - 0s 327us/sample - loss: 1.7278 - accuracy: 0.5585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7278463512385658, 0.5584862]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_eval, np.array(eval_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
