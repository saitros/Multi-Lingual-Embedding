{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# from googletrans import Translator\n",
    "from models.transformer import * \n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import requests\n",
    "import datetime\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ko_noun_dict.pkl', \"rb\") as f:\n",
    "    ko_dict = pickle.load(f)\n",
    "    \n",
    "with open('./data/en_noun_dict.pkl', \"rb\") as f:\n",
    "    en_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_data(ko_dict, en_dict):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = list(ko_dict.values())[i]\n",
    "        en_vector = list(en_dict.values())[i]\n",
    "        \n",
    "#         yield (ko_word, ko_vector, en_word, en_vector)\n",
    "        yield (ko_vector, en_vector)\n",
    "    \n",
    "def get_noun_data_2(ko_vec, en_vec):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = ko_vec[i]\n",
    "        en_vector = en_vec[i]\n",
    "        \n",
    "        yield ko_vector, en_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_noun_data_2, \n",
    "                              (tf.float64, tf.float64),\n",
    "                              (tf.TensorShape([300]), tf.TensorShape([300])),\n",
    "                               args=(list(ko_dict.values()), list(en_dict.values())))\n",
    "\n",
    "dataset = dataset.batch(128, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tf.Tensor(\n",
      "[[-2.34799996e-01  1.66319996e-01 -4.29280013e-01 ... -3.31140012e-01\n",
      "   1.89950004e-01 -2.73939997e-01]\n",
      " [ 1.88099995e-01  1.99169993e-01 -3.89319986e-01 ...  8.55629966e-02\n",
      "  -1.69309992e-02  4.76790011e-01]\n",
      " [ 1.88069999e-01 -4.09130007e-04  1.67890005e-02 ... -4.35310006e-01\n",
      "  -1.13710001e-01  3.76199991e-01]\n",
      " ...\n",
      " [-2.30690002e-01  1.01310000e-01 -6.93660021e-01 ... -3.70799989e-01\n",
      "   2.50369996e-01  1.08249998e+00]\n",
      " [-1.72539994e-01  2.10670009e-02  9.37939994e-03 ...  5.34860007e-02\n",
      "   3.27100009e-02 -3.39320004e-01]\n",
      " [-4.60009992e-01  1.11610003e-01 -7.62679994e-01 ... -2.70570010e-01\n",
      "  -3.07330012e-01 -1.95759997e-01]], shape=(128, 300), dtype=float64)\n",
      "2\n",
      "tf.Tensor(\n",
      "[[ 0.20111001  0.047298    0.19909    ... -0.17775001  0.27785\n",
      "  -0.51533002]\n",
      " [-0.054022    0.27379    -0.54825997 ... -0.37169001  0.35732001\n",
      "  -0.11435   ]\n",
      " [ 0.22186001  0.54045999 -0.20812    ... -0.28387001 -0.10013\n",
      "   1.10239995]\n",
      " ...\n",
      " [-0.24336     0.18984     0.12082    ...  0.12925     0.096424\n",
      "  -1.05970001]\n",
      " [ 0.084234    0.60346001 -0.51192999 ... -0.18993001  0.084478\n",
      "  -0.13107   ]\n",
      " [-0.38508999  0.24748001 -0.88313001 ... -0.47407001  0.28944001\n",
      "   0.12166   ]], shape=(128, 300), dtype=float64)\n",
      "3\n",
      "tf.Tensor(\n",
      "[[ 0.069365    0.081074   -0.34595999 ... -0.39383    -0.13131\n",
      "  -0.51528001]\n",
      " [ 0.33642     0.13410001 -0.58332998 ... -0.61918998 -0.01678\n",
      "   0.71961999]\n",
      " [-0.54816997  0.55360001 -0.58974999 ... -0.88779002  0.60895002\n",
      "  -0.31634   ]\n",
      " ...\n",
      " [-0.26420999 -0.015045    0.44530001 ...  0.055387   -0.16514\n",
      "  -0.30715999]\n",
      " [-0.67299998  0.044994   -0.59906    ... -0.3283      0.67276001\n",
      "  -0.081542  ]\n",
      " [-0.43507999  0.19220001 -1.12469995 ... -0.41536    -0.048432\n",
      "   0.1313    ]], shape=(128, 300), dtype=float64)\n",
      "4\n",
      "tf.Tensor(\n",
      "[[ 0.28496999 -0.12673999 -0.40979001 ... -0.15022001 -0.28692999\n",
      "  -0.48607001]\n",
      " [ 0.25073999  0.38328001 -0.13446    ... -0.090923    0.28685001\n",
      "   0.026655  ]\n",
      " [-0.0662     -0.43212    -0.048946   ... -0.19007    -0.39161\n",
      "  -0.33188   ]\n",
      " ...\n",
      " [-0.055835    0.19335     0.18912999 ...  0.44992    -0.25703999\n",
      "  -0.35725001]\n",
      " [-0.098217   -0.013002   -0.47588    ...  0.056034   -0.37687001\n",
      "  -0.068329  ]\n",
      " [-0.52610999  0.46963999 -2.23920012 ...  0.38834     0.05323\n",
      "  -0.25064   ]], shape=(128, 300), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for (batch, (inp, real)) in enumerate(dataset):\n",
    "    i += 1\n",
    "    print(i)\n",
    "    print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_layers=1, d_model=1, num_heads=1, dff=512, input_vocab_size=0, maximum_position_encoding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "num_layers = 1\n",
    "d_model = 1\n",
    "dff = 512\n",
    "num_head = 1\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.Huber()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(inp, real):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        output = encoder(inp, training=True, mask=None)\n",
    "        loss = loss_function(real, output)\n",
    "        \n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input shape (128, 300, 1)\n",
      "Scaled_attention Shape :  (128, 1, 300, 1)\n",
      "Scaled_attention Shape :  (128, 300, 1, 1)\n",
      "Concat attention Shape : (128, 300, 1)\n",
      "Epoch 1 Batch 0 Loss  0.0385\n",
      "Time taken for 1 epoch: 0.61 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss  0.0384\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.06 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.06 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss  0.0384\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss  0.0384\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss  0.0384\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss  0.0384\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tic = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    for (batch, (inp, real)) in enumerate(dataset):\n",
    "        train_step(inp, real)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result() : .4f}')\n",
    "                  \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - tic:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
