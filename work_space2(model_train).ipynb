{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# from googletrans import Translator\n",
    "from models.transformer import * \n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import requests\n",
    "import datetime\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ko_noun_dict.pkl', \"rb\") as f:\n",
    "    ko_dict = pickle.load(f)\n",
    "    \n",
    "with open('./data/en_noun_dict.pkl', \"rb\") as f:\n",
    "    en_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_data(ko_dict, en_dict):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = list(ko_dict.values())[i]\n",
    "        en_vector = list(en_dict.values())[i]\n",
    "        \n",
    "#         yield (ko_word, ko_vector, en_word, en_vector)\n",
    "        yield (ko_vector, en_vector)\n",
    "    \n",
    "def get_noun_data_2(ko_vec, en_vec):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = ko_vec[i]\n",
    "        en_vector = en_vec[i]\n",
    "        \n",
    "        yield ko_vector, en_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_noun_data_2, \n",
    "                              (tf.float64, tf.float64),\n",
    "                              (tf.TensorShape([300]), tf.TensorShape([300])),\n",
    "                               args=(list(ko_dict.values()), list(en_dict.values())))\n",
    "\n",
    "dataset = dataset.batch(128, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_layers=1, d_model=8, num_heads=8, dff=512, input_vocab_size=0, maximum_position_encoding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2000\n",
    "\n",
    "num_layers = 1\n",
    "d_model = 8\n",
    "dff = 512\n",
    "num_head = 8\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.Huber()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    print(real.shape)\n",
    "    print(\"pred shape : \",pred.shape)\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object_KLD = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def loss_function_KLD(real, pred):\n",
    "    print(real.shape)\n",
    "    print(\"pred shape : \",pred.shape)\n",
    "    loss = loss_object_KLD(real, pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./checkpoints/train(1000times embedding, Huber_loss)'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(inp, real):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        inp = inp * 1000 \n",
    "        real = real * 1000\n",
    "        output = encoder(inp, training=True, mask=None)\n",
    "#         output = output / 100\n",
    "        loss = loss_function(real, output)\n",
    "\n",
    "        \n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input shape (128, 300, 1)\n",
      "(128, 300, 1)\n",
      "Scaled_attention Shape :  (128, 8, 300, 1)\n",
      "Scaled_attention Shape :  (128, 300, 8, 1)\n",
      "Concat attention Shape : (128, 300, 8)\n",
      "(128, 300, 1)\n",
      "out1 shape : (128, 300, 1)\n",
      "out2 shape :  (128, 300, 1)\n",
      "(128, 300)\n",
      "pred shape :  (128, 300, 1)\n",
      "Epoch 1 Batch 0 Loss  217.7459\n",
      "Time taken for 1 epoch: 0.69 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss  217.7426\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss  217.7384\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss  217.7342\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss  217.7302\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-1\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss  217.7265\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss  217.7232\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss  217.7202\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss  217.7175\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss  217.7151\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-2\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss  217.7130\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss  217.7110\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss  217.7093\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss  217.7076\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss  217.7062\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-3\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss  217.7049\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss  217.7036\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss  217.7025\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss  217.7015\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss  217.7006\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-4\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss  217.6997\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss  217.6989\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss  217.6982\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss  217.6975\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss  217.6969\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-5\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss  217.6963\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss  217.6958\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss  217.6953\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss  217.6944\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-6\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss  217.6940\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss  217.6936\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss  217.6932\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss  217.6929\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss  217.6926\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-7\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss  217.6923\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss  217.6921\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss  217.6918\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss  217.6916\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss  217.6914\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-8\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss  217.6912\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss  217.6910\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss  217.6909\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss  217.6907\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss  217.6906\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-9\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss  217.6905\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss  217.6904\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss  217.6903\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss  217.6902\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss  217.6901\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-10\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss  217.6900\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss  217.6899\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss  217.6899\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss  217.6898\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss  217.6898\n",
      "Saving checkpoint for epoch 55 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-11\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss  217.6898\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss  217.6897\n",
      "Saving checkpoint for epoch 60 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-12\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss  217.6897\n",
      "Saving checkpoint for epoch 65 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-13\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss  217.6897\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss  217.6898\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss  217.6898\n",
      "Saving checkpoint for epoch 70 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-14\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 71 Batch 0 Loss  217.6898\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 72 Batch 0 Loss  217.6898\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 73 Batch 0 Loss  217.6898\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 74 Batch 0 Loss  217.6899\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 75 Batch 0 Loss  217.6899\n",
      "Saving checkpoint for epoch 75 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-15\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 76 Batch 0 Loss  217.6899\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 77 Batch 0 Loss  217.6900\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 78 Batch 0 Loss  217.6900\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 79 Batch 0 Loss  217.6901\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 80 Batch 0 Loss  217.6901\n",
      "Saving checkpoint for epoch 80 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-16\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 81 Batch 0 Loss  217.6902\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 82 Batch 0 Loss  217.6902\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 83 Batch 0 Loss  217.6902\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 84 Batch 0 Loss  217.6903\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 85 Batch 0 Loss  217.6903\n",
      "Saving checkpoint for epoch 85 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-17\n",
      "Time taken for 1 epoch: 0.14 secs\n",
      "\n",
      "Epoch 86 Batch 0 Loss  217.6904\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 87 Batch 0 Loss  217.6904\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 88 Batch 0 Loss  217.6905\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 89 Batch 0 Loss  217.6905\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 90 Batch 0 Loss  217.6906\n",
      "Saving checkpoint for epoch 90 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-18\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 91 Batch 0 Loss  217.6906\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 92 Batch 0 Loss  217.6907\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 93 Batch 0 Loss  217.6907\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 94 Batch 0 Loss  217.6908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 95 Batch 0 Loss  217.6908\n",
      "Saving checkpoint for epoch 95 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-19\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 96 Batch 0 Loss  217.6909\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 97 Batch 0 Loss  217.6909\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 98 Batch 0 Loss  217.6910\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 99 Batch 0 Loss  217.6910\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 100 Batch 0 Loss  217.6911\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-20\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 101 Batch 0 Loss  217.6911\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 102 Batch 0 Loss  217.6912\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 103 Batch 0 Loss  217.6912\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 104 Batch 0 Loss  217.6913\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 105 Batch 0 Loss  217.6913\n",
      "Saving checkpoint for epoch 105 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-21\n",
      "Time taken for 1 epoch: 0.14 secs\n",
      "\n",
      "Epoch 106 Batch 0 Loss  217.6914\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 107 Batch 0 Loss  217.6914\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 108 Batch 0 Loss  217.6915\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 109 Batch 0 Loss  217.6915\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 110 Batch 0 Loss  217.6916\n",
      "Saving checkpoint for epoch 110 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-22\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 111 Batch 0 Loss  217.6916\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 112 Batch 0 Loss  217.6917\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 113 Batch 0 Loss  217.6917\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 114 Batch 0 Loss  217.6918\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 115 Batch 0 Loss  217.6918\n",
      "Saving checkpoint for epoch 115 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-23\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 116 Batch 0 Loss  217.6919\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 117 Batch 0 Loss  217.6919\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 118 Batch 0 Loss  217.6920\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 119 Batch 0 Loss  217.6920\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 120 Batch 0 Loss  217.6921\n",
      "Saving checkpoint for epoch 120 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-24\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 121 Batch 0 Loss  217.6921\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 122 Batch 0 Loss  217.6922\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 123 Batch 0 Loss  217.6922\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 124 Batch 0 Loss  217.6922\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 125 Batch 0 Loss  217.6923\n",
      "Saving checkpoint for epoch 125 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-25\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 126 Batch 0 Loss  217.6923\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 127 Batch 0 Loss  217.6924\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 128 Batch 0 Loss  217.6924\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 129 Batch 0 Loss  217.6925\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 130 Batch 0 Loss  217.6925\n",
      "Saving checkpoint for epoch 130 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-26\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 131 Batch 0 Loss  217.6926\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 132 Batch 0 Loss  217.6926\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 133 Batch 0 Loss  217.6926\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 134 Batch 0 Loss  217.6927\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 135 Batch 0 Loss  217.6927\n",
      "Saving checkpoint for epoch 135 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-27\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 136 Batch 0 Loss  217.6927\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 137 Batch 0 Loss  217.6928\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 138 Batch 0 Loss  217.6928\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 139 Batch 0 Loss  217.6929\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 140 Batch 0 Loss  217.6929\n",
      "Saving checkpoint for epoch 140 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-28\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 141 Batch 0 Loss  217.6929\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 142 Batch 0 Loss  217.6930\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 143 Batch 0 Loss  217.6930\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 144 Batch 0 Loss  217.6931\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 145 Batch 0 Loss  217.6931\n",
      "Saving checkpoint for epoch 145 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-29\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 146 Batch 0 Loss  217.6931\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 147 Batch 0 Loss  217.6931\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 148 Batch 0 Loss  217.6932\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 149 Batch 0 Loss  217.6932\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 150 Batch 0 Loss  217.6932\n",
      "Saving checkpoint for epoch 150 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-30\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 151 Batch 0 Loss  217.6933\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 152 Batch 0 Loss  217.6933\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 153 Batch 0 Loss  217.6933\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 154 Batch 0 Loss  217.6934\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 155 Batch 0 Loss  217.6934\n",
      "Saving checkpoint for epoch 155 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-31\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 156 Batch 0 Loss  217.6934\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 157 Batch 0 Loss  217.6935\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 158 Batch 0 Loss  217.6935\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 159 Batch 0 Loss  217.6935\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 160 Batch 0 Loss  217.6936\n",
      "Saving checkpoint for epoch 160 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-32\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 161 Batch 0 Loss  217.6936\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 162 Batch 0 Loss  217.6936\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 163 Batch 0 Loss  217.6936\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 164 Batch 0 Loss  217.6937\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 165 Batch 0 Loss  217.6937\n",
      "Saving checkpoint for epoch 165 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-33\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 166 Batch 0 Loss  217.6937\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 167 Batch 0 Loss  217.6937\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 168 Batch 0 Loss  217.6938\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 169 Batch 0 Loss  217.6938\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 170 Batch 0 Loss  217.6938\n",
      "Saving checkpoint for epoch 170 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-34\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 171 Batch 0 Loss  217.6938\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 172 Batch 0 Loss  217.6938\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 173 Batch 0 Loss  217.6939\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 174 Batch 0 Loss  217.6939\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 175 Batch 0 Loss  217.6939\n",
      "Saving checkpoint for epoch 175 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-35\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 176 Batch 0 Loss  217.6939\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 177 Batch 0 Loss  217.6940\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 178 Batch 0 Loss  217.6940\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 179 Batch 0 Loss  217.6940\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 180 Batch 0 Loss  217.6940\n",
      "Saving checkpoint for epoch 180 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-36\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 181 Batch 0 Loss  217.6940\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 182 Batch 0 Loss  217.6941\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 183 Batch 0 Loss  217.6941\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 184 Batch 0 Loss  217.6941\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 185 Batch 0 Loss  217.6941\n",
      "Saving checkpoint for epoch 185 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-37\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 186 Batch 0 Loss  217.6941\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 187 Batch 0 Loss  217.6941\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 188 Batch 0 Loss  217.6942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 189 Batch 0 Loss  217.6942\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 190 Batch 0 Loss  217.6942\n",
      "Saving checkpoint for epoch 190 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-38\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 191 Batch 0 Loss  217.6942\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 192 Batch 0 Loss  217.6942\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 193 Batch 0 Loss  217.6942\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 194 Batch 0 Loss  217.6943\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 195 Batch 0 Loss  217.6943\n",
      "Saving checkpoint for epoch 195 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-39\n",
      "Time taken for 1 epoch: 0.14 secs\n",
      "\n",
      "Epoch 196 Batch 0 Loss  217.6943\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 197 Batch 0 Loss  217.6943\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 198 Batch 0 Loss  217.6943\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 199 Batch 0 Loss  217.6943\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 200 Batch 0 Loss  217.6944\n",
      "Saving checkpoint for epoch 200 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-40\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 201 Batch 0 Loss  217.6944\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 202 Batch 0 Loss  217.6944\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 203 Batch 0 Loss  217.6944\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 204 Batch 0 Loss  217.6944\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 205 Batch 0 Loss  217.6944\n",
      "Saving checkpoint for epoch 205 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-41\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 206 Batch 0 Loss  217.6944\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 207 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 208 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 209 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 210 Batch 0 Loss  217.6945\n",
      "Saving checkpoint for epoch 210 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-42\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 211 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 212 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 213 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 214 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 215 Batch 0 Loss  217.6945\n",
      "Saving checkpoint for epoch 215 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-43\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 216 Batch 0 Loss  217.6945\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 217 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 218 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 219 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 220 Batch 0 Loss  217.6946\n",
      "Saving checkpoint for epoch 220 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-44\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 221 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 222 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 223 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 224 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 225 Batch 0 Loss  217.6946\n",
      "Saving checkpoint for epoch 225 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-45\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 226 Batch 0 Loss  217.6946\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 227 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 228 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 229 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 230 Batch 0 Loss  217.6947\n",
      "Saving checkpoint for epoch 230 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-46\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 231 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 232 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 233 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 234 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 235 Batch 0 Loss  217.6947\n",
      "Saving checkpoint for epoch 235 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-47\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 236 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 237 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 238 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 239 Batch 0 Loss  217.6947\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 240 Batch 0 Loss  217.6947\n",
      "Saving checkpoint for epoch 240 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-48\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 241 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 242 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 243 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 244 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 245 Batch 0 Loss  217.6948\n",
      "Saving checkpoint for epoch 245 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-49\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 246 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 247 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 248 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 249 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 250 Batch 0 Loss  217.6948\n",
      "Saving checkpoint for epoch 250 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-50\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 251 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 252 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 253 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 254 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 255 Batch 0 Loss  217.6948\n",
      "Saving checkpoint for epoch 255 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-51\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 256 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 257 Batch 0 Loss  217.6948\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 258 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 259 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 260 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 260 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-52\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 261 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 262 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 263 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 264 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 265 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 265 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-53\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 266 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 267 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 268 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 269 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 270 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 270 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-54\n",
      "Time taken for 1 epoch: 0.16 secs\n",
      "\n",
      "Epoch 271 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 272 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 273 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 274 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 275 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 275 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-55\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 276 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 277 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 278 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 279 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 280 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 280 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-56\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 281 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 282 Batch 0 Loss  217.6949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 283 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 284 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 285 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 285 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-57\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 286 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 287 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 288 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 289 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 290 Batch 0 Loss  217.6949\n",
      "Saving checkpoint for epoch 290 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-58\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 291 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 292 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 293 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 294 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 295 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 295 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-59\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 296 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 297 Batch 0 Loss  217.6949\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 298 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 299 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 300 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 300 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-60\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 301 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 302 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 303 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 304 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 305 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 305 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-61\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 306 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 307 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 308 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 309 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 310 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 310 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-62\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 311 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 312 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 313 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 314 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 315 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 315 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-63\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 316 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 317 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 318 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 319 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 320 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 320 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-64\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 321 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 322 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 323 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 324 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 325 Batch 0 Loss  217.6950\n",
      "Saving checkpoint for epoch 325 at ./checkpoints/train(1000times embedding, Huber_loss)/ckpt-65\n",
      "Time taken for 1 epoch: 0.15 secs\n",
      "\n",
      "Epoch 326 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 327 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 328 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 329 Batch 0 Loss  217.6950\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-cbabe0baadd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m     if (context.executing_eagerly()\n\u001b[1;32m    417\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    592\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    598\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         dataset = _OptimizeDataset(dataset, static_optimizations,\n\u001b[0;32m--> 381\u001b[0;31m                                    static_optimization_configs)\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mautotune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations, optimization_configs)\u001b[0m\n\u001b[1;32m   4211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4214\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset\u001b[0;34m(input_dataset, optimizations, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   3625\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3627\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   3628\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tic = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    for (batch, (inp, real)) in enumerate(dataset):\n",
    "        train_step(inp, real)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result() : .4f}')\n",
    "                  \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - tic:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
