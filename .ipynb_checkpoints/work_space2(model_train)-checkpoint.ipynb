{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# from googletrans import Translator\n",
    "from models.transformer import * \n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import requests\n",
    "import datetime\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ko_noun_dict.pkl', \"rb\") as f:\n",
    "    ko_dict = pickle.load(f)\n",
    "    \n",
    "with open('./data/en_noun_dict.pkl', \"rb\") as f:\n",
    "    en_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_data(ko_dict, en_dict):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = list(ko_dict.values())[i]\n",
    "        en_vector = list(en_dict.values())[i]\n",
    "        \n",
    "#         yield (ko_word, ko_vector, en_word, en_vector)\n",
    "        yield (ko_vector, en_vector)\n",
    "    \n",
    "def get_noun_data_2(ko_vec, en_vec):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = ko_vec[i]\n",
    "        en_vector = en_vec[i]\n",
    "        \n",
    "        yield ko_vector, en_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_noun_data_2, \n",
    "                              (tf.float64, tf.float64),\n",
    "                              (tf.TensorShape([300]), tf.TensorShape([300])),\n",
    "                               args=(list(ko_dict.values()), list(en_dict.values())))\n",
    "\n",
    "dataset = dataset.batch(128, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_88 = Encoder(num_layers=1, d_model=8, num_heads=8, dff=512, input_vocab_size=0, maximum_position_encoding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "num_layers = 1\n",
    "d_model = 8\n",
    "dff = 512\n",
    "num_head = 8\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.Huber()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./checkpoints/train_88\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder_100,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(inp, real):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        inp = inp * 100 \n",
    "        real = real * 100\n",
    "        output = encoder(inp, training=True, mask=None)\n",
    "#         output = output / 100\n",
    "        loss = loss_function(real, output)\n",
    "\n",
    "        \n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input shape (128, 300, 1)\n",
      "Scaled_attention Shape :  (128, 1, 300, 1)\n",
      "Scaled_attention Shape :  (128, 300, 1, 1)\n",
      "Concat attention Shape : (128, 300, 1)\n",
      "Epoch 1 Batch 0 Loss  21.3269\n",
      "Time taken for 1 epoch: 0.63 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss  21.3269\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss  21.3269\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss  21.3269\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss  21.3268\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train_88/ckpt-49\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss  21.3268\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss  21.3268\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss  21.3268\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss  21.3267\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss  21.3267\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train_88/ckpt-50\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss  21.3267\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss  21.3267\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss  21.3267\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss  21.3267\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss  21.3266\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train_88/ckpt-51\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss  21.3266\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss  21.3266\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss  21.3266\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss  21.3266\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss  21.3266\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train_88/ckpt-52\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss  21.3265\n",
      "Saving checkpoint for epoch 25 at ./checkpoints/train_88/ckpt-53\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss  21.3265\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss  21.3264\n",
      "Saving checkpoint for epoch 30 at ./checkpoints/train_88/ckpt-54\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss  21.3264\n",
      "Saving checkpoint for epoch 35 at ./checkpoints/train_88/ckpt-55\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss  21.3264\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss  21.3263\n",
      "Saving checkpoint for epoch 40 at ./checkpoints/train_88/ckpt-56\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss  21.3263\n",
      "Saving checkpoint for epoch 45 at ./checkpoints/train_88/ckpt-57\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss  21.3263\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss  21.3262\n",
      "Saving checkpoint for epoch 50 at ./checkpoints/train_88/ckpt-58\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss  21.3262\n",
      "Saving checkpoint for epoch 55 at ./checkpoints/train_88/ckpt-59\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss  21.3262\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss  21.3262\n",
      "Saving checkpoint for epoch 60 at ./checkpoints/train_88/ckpt-60\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss  21.3261\n",
      "Saving checkpoint for epoch 65 at ./checkpoints/train_88/ckpt-61\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss  21.3261\n",
      "Saving checkpoint for epoch 70 at ./checkpoints/train_88/ckpt-62\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 71 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 72 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 73 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 74 Batch 0 Loss  21.3261\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 75 Batch 0 Loss  21.3260\n",
      "Saving checkpoint for epoch 75 at ./checkpoints/train_88/ckpt-63\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 76 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 77 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 78 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 79 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 80 Batch 0 Loss  21.3260\n",
      "Saving checkpoint for epoch 80 at ./checkpoints/train_88/ckpt-64\n",
      "Time taken for 1 epoch: 0.10 secs\n",
      "\n",
      "Epoch 81 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 82 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 83 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 84 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 85 Batch 0 Loss  21.3260\n",
      "Saving checkpoint for epoch 85 at ./checkpoints/train_88/ckpt-65\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 86 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 87 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 88 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 89 Batch 0 Loss  21.3260\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 90 Batch 0 Loss  21.3259\n",
      "Saving checkpoint for epoch 90 at ./checkpoints/train_88/ckpt-66\n",
      "Time taken for 1 epoch: 0.08 secs\n",
      "\n",
      "Epoch 91 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 92 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 93 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 94 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 95 Batch 0 Loss  21.3259\n",
      "Saving checkpoint for epoch 95 at ./checkpoints/train_88/ckpt-67\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 96 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 97 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 98 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 99 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 100 Batch 0 Loss  21.3259\n",
      "Saving checkpoint for epoch 100 at ./checkpoints/train_88/ckpt-68\n",
      "Time taken for 1 epoch: 0.09 secs\n",
      "\n",
      "Epoch 101 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n",
      "Epoch 102 Batch 0 Loss  21.3259\n",
      "Time taken for 1 epoch: 0.07 secs\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-cbabe0baadd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    416\u001b[0m     if (context.executing_eagerly()\n\u001b[1;32m    417\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[0;32m--> 418\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    592\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    617\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2694\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   2695\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2696\u001b[0;31m         tld.op_callbacks, dataset, iterator)\n\u001b[0m\u001b[1;32m   2697\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tic = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    for (batch, (inp, real)) in enumerate(dataset):\n",
    "        train_step(inp, real)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result() : .4f}')\n",
    "                  \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - tic:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
