{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# from googletrans import Translator\n",
    "from models.transformer import * \n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import requests\n",
    "import datetime\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/ko_noun_dict.pkl', \"rb\") as f:\n",
    "    ko_dict = pickle.load(f)\n",
    "    \n",
    "with open('./data/en_noun_dict.pkl', \"rb\") as f:\n",
    "    en_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_data(ko_dict, en_dict):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = list(ko_dict.values())[i]\n",
    "        en_vector = list(en_dict.values())[i]\n",
    "        \n",
    "        yield (ko_vector, en_vector)\n",
    "    \n",
    "def get_noun_data_2(ko_vec, en_vec):\n",
    "    for i in range(len(en_dict)):\n",
    "        ko_vector = ko_vec[i]\n",
    "        en_vector = en_vec[i]\n",
    "        \n",
    "        yield ko_vector, en_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(get_noun_data_2, \n",
    "                              (tf.float64, tf.float64),\n",
    "                              (tf.TensorShape([300]), tf.TensorShape([300])),\n",
    "                               args=(list(ko_dict.values()), list(en_dict.values())))\n",
    "\n",
    "dataset = dataset.batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_layers=1, d_model=8, num_heads=8, dff=512, input_vocab_size=0, maximum_position_encoding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20000\n",
    "\n",
    "num_layers = 1\n",
    "d_model = 8\n",
    "dff = 512\n",
    "num_head = 8\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.Huber()\n",
    "loss_object_KLD = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.SUM)\n",
    "loss_object_MSE = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    print(real.shape)\n",
    "    print(\"pred shape : \",pred.shape)\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_function_KLD(real, pred):\n",
    "    print(real.shape)\n",
    "    print(\"pred shape : \",pred.shape)\n",
    "    loss = loss_object_KLD(real, pred)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_function_MSE(real, pred):\n",
    "    loss = loss_object_MSE(real, pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./checkpoints/train(1times embedding, MSE)'\n",
    "\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(inp, real):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        inp = inp * 1\n",
    "        real = real * 1\n",
    "        output = encoder(inp, training=True, mask=None)\n",
    "#         output = output / 100\n",
    "        loss = loss_function_MSE(real, output)\n",
    "\n",
    "        \n",
    "    gradients = tape.gradient(loss, encoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, encoder.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model Input shape (32, 300, 1)\n",
      "(32, 300, 1)\n",
      "Scaled_attention Shape :  (32, 8, 300, 1)\n",
      "Scaled_attention Shape :  (32, 300, 8, 1)\n",
      "Concat attention Shape : (32, 300, 8)\n",
      "(32, 300, 1)\n",
      "Model Input shape (32, 300, 1)\n",
      "(32, 300, 1)\n",
      "Scaled_attention Shape :  (32, 8, 300, 1)\n",
      "Scaled_attention Shape :  (32, 300, 8, 1)\n",
      "Concat attention Shape : (32, 300, 8)\n",
      "(32, 300, 1)\n",
      "Epoch 1 Batch 0 Loss  0.0690\n",
      "Epoch 1 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 2.07 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss  0.0689\n",
      "Epoch 2 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss  0.0689\n",
      "Epoch 3 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss  0.0689\n",
      "Epoch 4 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss  0.0689\n",
      "Epoch 5 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss  0.0689\n",
      "Epoch 6 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss  0.0689\n",
      "Epoch 7 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss  0.0689\n",
      "Epoch 8 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss  0.0689\n",
      "Epoch 9 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss  0.0689\n",
      "Epoch 10 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss  0.0689\n",
      "Epoch 11 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss  0.0689\n",
      "Epoch 12 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss  0.0689\n",
      "Epoch 13 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss  0.0689\n",
      "Epoch 14 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss  0.0689\n",
      "Epoch 15 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss  0.0689\n",
      "Epoch 16 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss  0.0689\n",
      "Epoch 17 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss  0.0689\n",
      "Epoch 18 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss  0.0689\n",
      "Epoch 19 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss  0.0689\n",
      "Epoch 20 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss  0.0689\n",
      "Epoch 21 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss  0.0689\n",
      "Epoch 22 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss  0.0689\n",
      "Epoch 23 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss  0.0689\n",
      "Epoch 24 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss  0.0689\n",
      "Epoch 25 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss  0.0689\n",
      "Epoch 26 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss  0.0689\n",
      "Epoch 27 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss  0.0689\n",
      "Epoch 28 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss  0.0689\n",
      "Epoch 29 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss  0.0689\n",
      "Epoch 30 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 31 Batch 0 Loss  0.0689\n",
      "Epoch 31 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 32 Batch 0 Loss  0.0689\n",
      "Epoch 32 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 33 Batch 0 Loss  0.0689\n",
      "Epoch 33 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 34 Batch 0 Loss  0.0689\n",
      "Epoch 34 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 35 Batch 0 Loss  0.0689\n",
      "Epoch 35 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 36 Batch 0 Loss  0.0689\n",
      "Epoch 36 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 37 Batch 0 Loss  0.0689\n",
      "Epoch 37 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 38 Batch 0 Loss  0.0689\n",
      "Epoch 38 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 39 Batch 0 Loss  0.0689\n",
      "Epoch 39 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 40 Batch 0 Loss  0.0689\n",
      "Epoch 40 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 41 Batch 0 Loss  0.0689\n",
      "Epoch 41 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 42 Batch 0 Loss  0.0689\n",
      "Epoch 42 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 43 Batch 0 Loss  0.0689\n",
      "Epoch 43 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 44 Batch 0 Loss  0.0689\n",
      "Epoch 44 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 45 Batch 0 Loss  0.0689\n",
      "Epoch 45 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 46 Batch 0 Loss  0.0689\n",
      "Epoch 46 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 47 Batch 0 Loss  0.0689\n",
      "Epoch 47 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 48 Batch 0 Loss  0.0689\n",
      "Epoch 48 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 49 Batch 0 Loss  0.0689\n",
      "Epoch 49 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 50 Batch 0 Loss  0.0689\n",
      "Epoch 50 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 51 Batch 0 Loss  0.0689\n",
      "Epoch 51 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 52 Batch 0 Loss  0.0689\n",
      "Epoch 52 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 53 Batch 0 Loss  0.0689\n",
      "Epoch 53 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 54 Batch 0 Loss  0.0689\n",
      "Epoch 54 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 55 Batch 0 Loss  0.0689\n",
      "Epoch 55 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 56 Batch 0 Loss  0.0689\n",
      "Epoch 56 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 57 Batch 0 Loss  0.0689\n",
      "Epoch 57 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 58 Batch 0 Loss  0.0689\n",
      "Epoch 58 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.11 secs\n",
      "\n",
      "Epoch 59 Batch 0 Loss  0.0689\n",
      "Epoch 59 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 60 Batch 0 Loss  0.0689\n",
      "Epoch 60 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 61 Batch 0 Loss  0.0689\n",
      "Epoch 61 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 62 Batch 0 Loss  0.0689\n",
      "Epoch 62 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 63 Batch 0 Loss  0.0689\n",
      "Epoch 63 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 64 Batch 0 Loss  0.0689\n",
      "Epoch 64 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 65 Batch 0 Loss  0.0689\n",
      "Epoch 65 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 66 Batch 0 Loss  0.0689\n",
      "Epoch 66 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 67 Batch 0 Loss  0.0689\n",
      "Epoch 67 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 68 Batch 0 Loss  0.0689\n",
      "Epoch 68 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 69 Batch 0 Loss  0.0689\n",
      "Epoch 69 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 70 Batch 0 Loss  0.0689\n",
      "Epoch 70 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 71 Batch 0 Loss  0.0689\n",
      "Epoch 71 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 72 Batch 0 Loss  0.0689\n",
      "Epoch 72 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 73 Batch 0 Loss  0.0689\n",
      "Epoch 73 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 74 Batch 0 Loss  0.0689\n",
      "Epoch 74 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 75 Batch 0 Loss  0.0689\n",
      "Epoch 75 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 76 Batch 0 Loss  0.0689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 77 Batch 0 Loss  0.0689\n",
      "Epoch 77 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 78 Batch 0 Loss  0.0689\n",
      "Epoch 78 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.13 secs\n",
      "\n",
      "Epoch 79 Batch 0 Loss  0.0689\n",
      "Epoch 79 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 80 Batch 0 Loss  0.0689\n",
      "Epoch 80 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 81 Batch 0 Loss  0.0689\n",
      "Epoch 81 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 82 Batch 0 Loss  0.0689\n",
      "Epoch 82 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 83 Batch 0 Loss  0.0689\n",
      "Epoch 83 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 84 Batch 0 Loss  0.0689\n",
      "Epoch 84 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 85 Batch 0 Loss  0.0689\n",
      "Epoch 85 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 86 Batch 0 Loss  0.0689\n",
      "Epoch 86 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 87 Batch 0 Loss  0.0689\n",
      "Epoch 87 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 88 Batch 0 Loss  0.0689\n",
      "Epoch 88 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 89 Batch 0 Loss  0.0689\n",
      "Epoch 89 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 90 Batch 0 Loss  0.0689\n",
      "Epoch 90 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 91 Batch 0 Loss  0.0689\n",
      "Epoch 91 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 92 Batch 0 Loss  0.0689\n",
      "Epoch 92 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 93 Batch 0 Loss  0.0689\n",
      "Epoch 93 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 94 Batch 0 Loss  0.0689\n",
      "Epoch 94 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 95 Batch 0 Loss  0.0689\n",
      "Epoch 95 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 96 Batch 0 Loss  0.0689\n",
      "Epoch 96 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 97 Batch 0 Loss  0.0689\n",
      "Epoch 97 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 98 Batch 0 Loss  0.0689\n",
      "Epoch 98 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 99 Batch 0 Loss  0.0689\n",
      "Epoch 99 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 100 Batch 0 Loss  0.0689\n",
      "Epoch 100 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 101 Batch 0 Loss  0.0689\n",
      "Epoch 101 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 102 Batch 0 Loss  0.0689\n",
      "Epoch 102 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 103 Batch 0 Loss  0.0689\n",
      "Epoch 103 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 104 Batch 0 Loss  0.0689\n",
      "Epoch 104 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 105 Batch 0 Loss  0.0689\n",
      "Epoch 105 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 106 Batch 0 Loss  0.0689\n",
      "Epoch 106 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 107 Batch 0 Loss  0.0689\n",
      "Epoch 107 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 108 Batch 0 Loss  0.0689\n",
      "Epoch 108 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 109 Batch 0 Loss  0.0689\n",
      "Epoch 109 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 110 Batch 0 Loss  0.0689\n",
      "Epoch 110 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 111 Batch 0 Loss  0.0689\n",
      "Epoch 111 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 112 Batch 0 Loss  0.0689\n",
      "Epoch 112 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 113 Batch 0 Loss  0.0689\n",
      "Epoch 113 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 114 Batch 0 Loss  0.0689\n",
      "Epoch 114 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 115 Batch 0 Loss  0.0689\n",
      "Epoch 115 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 116 Batch 0 Loss  0.0689\n",
      "Epoch 116 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 117 Batch 0 Loss  0.0689\n",
      "Epoch 117 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 118 Batch 0 Loss  0.0689\n",
      "Epoch 118 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 119 Batch 0 Loss  0.0689\n",
      "Epoch 119 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 120 Batch 0 Loss  0.0689\n",
      "Epoch 120 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 121 Batch 0 Loss  0.0689\n",
      "Epoch 121 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 122 Batch 0 Loss  0.0689\n",
      "Epoch 122 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 123 Batch 0 Loss  0.0689\n",
      "Epoch 123 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 124 Batch 0 Loss  0.0689\n",
      "Epoch 124 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 125 Batch 0 Loss  0.0689\n",
      "Epoch 125 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 126 Batch 0 Loss  0.0689\n",
      "Epoch 126 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 127 Batch 0 Loss  0.0689\n",
      "Epoch 127 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 128 Batch 0 Loss  0.0689\n",
      "Epoch 128 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 129 Batch 0 Loss  0.0689\n",
      "Epoch 129 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 130 Batch 0 Loss  0.0689\n",
      "Epoch 130 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 131 Batch 0 Loss  0.0689\n",
      "Epoch 131 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 132 Batch 0 Loss  0.0689\n",
      "Epoch 132 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 133 Batch 0 Loss  0.0689\n",
      "Epoch 133 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 134 Batch 0 Loss  0.0689\n",
      "Epoch 134 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 135 Batch 0 Loss  0.0689\n",
      "Epoch 135 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 136 Batch 0 Loss  0.0689\n",
      "Epoch 136 Batch 10 Loss  0.0789\n",
      "Time taken for 1 epoch: 0.12 secs\n",
      "\n",
      "Epoch 137 Batch 0 Loss  0.0689\n",
      "Epoch 137 Batch 10 Loss  0.0789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-529eb4a1c280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2468\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2469\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tic = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    for (batch, (inp, real)) in enumerate(dataset):\n",
    "        train_step(inp, real)\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result() : .4f}')\n",
    "                  \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Time taken for 1 epoch: {time.time() - tic:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
